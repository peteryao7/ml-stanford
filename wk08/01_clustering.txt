Unsupervised Learning: Introduction

In supervised learning, we're given a labeled training set and our goal is to
find a boundary separating the positive and negative labeled examples. We want
to fit a hypothesis to it.

In unsupervised learning, we're given data with no labels associated with it.
It's just a set of points and no labels mapped onto a graph of x_1 vs. x_2.
We're asking the algorithm to find some sort of structure to it.

In this case, it looks like we have 2 groups of points bunched together. Those
are clusters. We can use a clustering algorithm to identify clusters.

Applications of clustering
    - market segmentation
    - social network analysis, like looking at info of groups on FB or G+ or LI
    - organize computing clusters or data centers to find out how to design
      your network or data center communications
    - astronomical data analysis - galaxy formation

==================================================

K-means Algorithm

Suppose you have a graph of unlabeled data as before. If you run the K-means clustering
algorithm, you would first identify 2 cluster centroids, color them red or blue. Then,
the algorithm iterates repeatedly through these 2 steps:
    - assignment
        - in an inner loop of k-means, it will go through each example and depending on
          whether it's closer to a centroid compared to the other, it's going to assign each
          data point to a centroid
        - then it will color each data point depending on which centroid it's closer to,
          clustering them together to a centroid
    - move centroid step
        - move the centroids to the average points of all the locations of each color
        - look at all red/blue dots, compute the mean, and move the centroids there
As you run additional iterations, the centroids will stop moving after a while and you will
reach separated groups of clusters.

Input:
    - K (# clusters)
    - Training set {x^(1), x^(2), ..., x^(m)} (no y since we're doing unsupervised learning)

    x^(i) ∈ R^n (drop x_0 = 1 convention)

---

Procedure of k-means algorithm

Randomly initialize K cluster centroids μ_1, μ_2, ..., μ_K ∈ R^n

repeat {
    for i = 1:m
        c^(i) := index (from 1 to K) of cluster centroid closest to x^(i)
        % find min k ||x^(i) - μ_k||
    end
    for k = 1:K
        μ_k := average (mean) of points assigned to cluster k
        % ex. μ_2 has points x^(1), x^(5), x^(6), x^(10) assigned to it
        % and c^(1) = 2, c^(5) = 2, c^(6) = 2, c^(10) = 2, meaning all 4 points got assigned to cluster 2
        % then here, we'll compute the average of their distance
        % μ_2 = 1/4 * (x^(1) + x^(5) + x^(6) + x^(10)) ∈ R^n
        % it's possible for a centroid to have no points, but it doesn't happen often
    end
}

-----

K-means for non-separated clusters

Say you're a t-shirt manufacturer and you want to sell t-shirts to a certain population.
You collect data on height and weight, and they seem to be positively correlated with no clusters.
You want to make 3 sizes of shirts, S/M/L. 

K-means can still cluster them appropriately, even if the clusters aren't very apparent.
It's similar to market segmentation, where you can design a product for different sizes to suit
different sub-populations.

==================================================

Optimization Objective

Most of our supervised learning algos like linear regression and logistic regression, have an
optimization object or some cost function that the algo wants to minimize. 

K-means has its own optimization objective as well, to ensure it's running correctly and
how we can help k-means find better clusters and avoid local optima.

-----

Notation:

c^(i) = index of cluster (1,2,...,K) to which example x^(i) is currently assigned
μ_k = cluster centroid k (μ_k ∈ R^n)
μ_(c^(i)) = cluster centroid of cluster to which example x^(i) has been assigned

Say x^(i) has been assigned to cluster 5, so c^(i) = 5 and μ_(c^(i)) = μ_5.

-----

The optimization objective:

J(c^(1), ..., c^(m), μ_1, ..., μ_K) = 1/m * sum(i=1, m) ||x^(i) - μ_(c^(i))||^2
min c^(1),...,c^(m) and μ_1,...,μ_K of J(c^(1),...,c^(m),μ_1,...,μ_K)

Also known as the distortion of the algorithm.

Randomly initialize K cluster centroids μ_1, μ_2, ..., μ_K ∈ R^n

repeat {
    for i = 1:m
        c^(i) := index (from 1 to K) of cluster centroid closest to x^(i)
        % this is minimizing J(...) with respect to c^(1),...,c^(m) holding μ_1,...,μ_K fixed
    end
    for k = 1:K
        μ_k := average (mean) of points assigned to cluster k
        % minimize J(...) with respect to μ_1,...,μ_K
    end
}