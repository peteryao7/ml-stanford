Principal Component Analysis Problem Formulation

Principal component analysis (PCA) is the most commonly used algorithm
for dimensionality reduction.

-----

Say we have a 2D plot of data and we want to reduce it to 1D. If we draw a decent-looking
line through our data, what we find is that the distance between each point and the 
projected line is pretty small.

What PCA does is that it tries to find a lower dimensional surface, a line in this case, onto
which to project the data so that the sum of squares of those lines is minimized. 

Before applying PCA, it's common to first perform mean normalization at feature scaling so
that the features x_1 and x_2 have 0 mean, and have comparable ranges of values.

-----

Formal definition:

Reduce from 2D to 1D: Find a direction (a vector u^(1) ∈ R^n) onto which to project the data
so as to minimize the projection error.

Reduce from n-D to k-D: Find k vectors u^(1), u^(2), ..., u^(k) onto which to project the data
so as to minimize the projection error.

Ex. to reduce 3D to 2D, we'd pick vectors u^(1) and u^(2) that define a 2D surface onto which
you'd want to project your data.

-----

How does PCA relate to linear regression?

Even though they look a lot like each other, they are quite different.

In linear regression, we're fitting a straight line to minimize the squared magnitudes of
the y-values between the training data and the model.

In PCA, we're fitting a straight line to minimize the magnitude of the orthogonal distances,
so those lines are not vertical, they're perpendicular to the straight line we drew.

PCA has no special y we're trying to predict; we have a list of features and we're treating
all of them equally.

==================================================

PCA Algorithm

Given a training set x^(1), x^(2), ..., x^(m)

Data preprocessing (featuring scaling/mean normalization) steps:
    - μ_j = 1/m * sum(i=1, m) x_j^(i)
    - replace each x_j^(i) with x_j - μ_j
    - if different features on different scales (e.g. x_1 = size of house, x_2 = # bedrooms),
      scale features to have comparable range of values
        - x_j^(i) <- (x_j^(i) - μ_j) / s_j, the standard deviation of feature j

-----

When reducing data from 2D to 1D, we can construct a vector u^(1) and use it to project 
x^(i) ∈ R^2 -> z^(i) ∈ R using the location of x^(i).

When reducing data from 3D to 2D, we can construct vectors u^(1) and u^(2) to construct
a plane and project x^(i) ∈ R^3 -> z^(i) ∈ R^2

Now, we need to know how to compute 2 things: the u vectors and z values.

-----

Reducing data from n-dimensions to k-dimensions

Compute "covariance matrix":
Σ = 1/m * sum(i=1, n) (x^(i))*(x^(i))^T

Σ will be an n x n matrix, since x^(i) is n x 1 and x^(i)^T will be 1 x n.

Compute eigenvectors of matrix Σ:
[U,S,V] = svd(Sigma); where svd is singular value decomposition

U ∈ R^(n x n), and the columns are comprised of u^(1), u^(2), ..., u^(m).
Since we only want the first k dimensions, we'll only take the first k columns of U, up to u^(k).

eig(Sigma) can also compute the same thing, but svd is a little more numerically stable.
This is because Σ is symmetric positive semidefinite.

-----

Now we need to take our original data set x ∈ R^(N) and find a lower dimensional 
representation z ∈ R^(k) for this data.

Take the first k columns of the U matrix, making an n x k matrix named U_reduce, and we'll set z
equal to U_reduce^T * x. This results in (u^(1))^T, ..., (u^(k))^T in rows as a k x n matrix, and 
x is n x 1, so we get a z as a k x 1 matrix, or z ∈ R^k.

-----

Summary:

After mean normalization (ensure every feature has zero mean) and optionally feature scaling:
Sigma = 1/m * sum(i=1, m) (x^(i)) * (x^(i))^T % Sigma = (1/m) * X' * X;
[U,S,V] = svd(Sigma);
Ureduce = U(:,1:k);
z = Ureduce' * x;