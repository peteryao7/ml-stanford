Problem Motivation of Anomaly Detection

Example:

Aircraft engine features: x_1 = heat generated, x_2 = vibration intensity ...
Dataset: {x^(1),x^(2),...,x^(m)}

Then we plot all the engines in an unlabeled plot.

Suppose you have a new engine x_test and you want to see if the engine is 
anomalous in any way and requires further testing.

If the engine is inside the cluster, then we think it's ok. 
If it were outside the cluster, then maybe it's an anomaly.

For model p(x), if p(x_test) < ε, flag as an anomaly
               if p(x_test) >= ε, OK

So it would be like rings, or contour lines, surrounding the central point
of the cluster of points. If the new point is 3 rings outside, then 
consider it an anomaly.

-----

Fraud detection:
    - x^(i) = features of user i's activities
    - Model p(x) from data
    - Identify unusual users by checking which have p(x) < ε

Examples of anomaly detection:
    - detecting user behavior on your website, keeping track of pages visited, # transactions,
      typing speed etc.
        - if you find users as an anomaly by checking p(x) < ε, isolate them for further investigation
    - manufacturing

Monitoring computers in a data center
    x^(i) = features of machine i
    x_1 = memory use, x_2 = # of disk accesses/sec, x_3 = CPU load, x_4 = CPU load / network traffic etc.

If you're flagging too many things as an anomaly, decrease ε and make the restriction more strict.
If you're flagging no anomalies and can see them yourself, increase ε.

==================================================

Gaussian Distribution (Normal Distribution)

Say x ∈ R. x is a distributed Gaussian with mean μ and variance σ^2.

x ~ N(μ,σ^2)

If we plot the Gaussian distribution, it'll look like the bell curve, parameterized by μ and σ.
The center of the bell-shaped curve is μ, and the width (one standard deviation) by σ. 

Note the "width" covers the distance from μ to one standard deviation, or half the whole curve.

The function of the graph:
p(x;μ,σ^2) = 1 / (sqrt(2*pi) * σ) * exp(- (x-μ)^2 / (2*σ^2))

σ is the standard deviation, where σ^2 is the variance.

-----

Examples

If μ = 0, σ = 1, then the curve is centered at 0 and the width of the Gaussian is 1.
If μ = 0, σ = 0.5, then the curve is centered at 0 and the width of the Gaussian is 0.5,
    so the curve is narrower and twice as tall since the area under the curve must integrate to 1.
If μ = 0, σ = 2, then the curve is centered at 0 and the width of the Gaussian is 2,
    so the curve is wider and half as tall.
If μ = 3, σ = 0.5, then the curve shifts to the right by 3 and the width of the Gaussian is 0.5.

-----

Parameter estimation

Dataset: {x^(1),x^(2),...,x^(m)} x^(i) ∈ R

Let's say I suspect these examples came from a Gaussian distribution, so x^(i) ~ N(μ,σ^2), but
we don't know what μ and σ are.

You can center the distribution around where the data clusters up the most (μ) and eye the
width (σ) depending on when the cluster begins to dissipate.

μ = 1/m * sum(i=1, m) x^(i)
σ^2 = 1/m * sum(i=1, m) (x^(i) - μ)^2
μ is the square difference and σ^2 averages the squared differences.

These estimates of μ and σ^2 are also the maximum likelihood estimates of their primes.

==================================================

Applying Gaussian Distribution to an Anomaly Detection Algorithm

Training set: {x^(1),x^(2),...,x^(m)}, x ∈ R^n

Assume x_1 ~ N(μ_1,σ_1^2), x_2 ~ N(μ_2,σ_2^2) etc.

p(x) = p(x_1;μ_1,σ_1^2) * p(x_2;μ_2,σ_2^2) * p(x_3;μ_3,σ_3^2) * ... * p(x_n;μ_n,σ_n^2)
     = prod(j=1, n) p(x_j;μ_j,σ_j^2)

p(x) corresponds to an independent assumption on the values of the features x_1 to x_n, 
but in practice, it doesn't matter if they're independent or not.

-----

Anomaly detection algorithm
    - choose features x_i that you think might be indicative of anomalous examples
    - fit params μ_1,...,μ_n,σ_1^2,...,σ_n^2
      where μ_j = 1/m * sum(i=1, m) x_j^(i)
            σ_j^2 = 1/m * sum(i=1, m) (x_j^(i) - μ_j)^2
    - given new example x, compute p(x):
            p(x) = prod(j=1, n) p(x_j;μ_j,σ_j^2)
                 = prod(j=1, n) 1 / (sqrt(2*pi) * σ_j) * exp(-(x_j - μ_j)^2 / (2*σ_j^2))
    - anomaly if p(x) < ε

-----

Applying the algorithm

Looking at a plot of x_1 vs. x_2, the data kind of clumps around x_1 = 5, so μ_1 = 5, and the big cluster
stops around 2 away from its center, so σ_1 = 2.

Looking at the same plot respect to the y-axis, the data clumps around x_2 = 3, so μ_2 = 3 and dissipates
1 away from the center, so σ_2 = 1.

We can plot p(x_1;μ_1,σ_1) and p(x_2;μ_2,σ_2), then plot it as a 3D graph. 
The height of the 3D surface is (x_1;μ_1,σ_1) * p(x_2;μ_2,σ_2).

Then we pick a ε, say 0.02, then start plugging x_test values into p(x) and compare them against ε.