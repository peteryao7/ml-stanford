Problem Description and Pipeline

Photo OCR - Photo Optical Character Recognition
With the growth of digital photography through cell phones, we now have tons of visual
pictures that we take all over the place. One thing that has interested devs is how
to get our computers to understand that content better.

Given an image, it'd be nice if a computer can read the text in the image, and if you're
searching for "antique mall" or "lula b's, it'll pull up this picture of an antique mall 
instead of needing to go through hundreds of images.

You could also provide a blind person a camera that can look at what's in front of them,
and just tell them the words that may be on the street sign in front of them.

Imagine if your car could read street signs and help you navigate to your destination.

-----

Pipeline

1) Text detection - Lula B's Antique Mall, and put a box around it
2) Character segmentation - into each character
3) Character classification - decipher which character is what

Hopefully, the algorithm can read the whole line of text.

If the algorithm reads a misspelling, like c1eaning, when there can also be a spell check
system to correct that as cleaning.

Our pipeline:
Image -> text detection -> character segmentation -> character recognition

-----

For many complex ML systems, these pipelines are common, where you can have multiple modules.
For this example, text detection, character segmentation, and character recognition are 
modules can be an ML component that act on some sort of data to produce the output you want.

If you're designing an ML system, one of the most important decisions is what modules you
want in your pipeline and what its breakdown is. Each module could require 1-5 engineers
to work on.

==================================================

Sliding Windows

How the individual components of the OCR pipeline works - the sliding window classifier.

-----

Text detection - finding regions of text that appear in the image
    - the red rectangles that identify text can have different aspect ratios

Note: aspect ratio is the ratio between the height and width of the rectangles.

Let's look first at an easier problem - pedestrian detection
    - seems simpler since the aspect ratio for the pedestrians are quite similar, so we
      can use a fixed aspect ratio
    - pedestrians can still be farther away from the camera and be smaller in size, but
      in general the aspect ratios are the same

Supervised learning for pedestrian detection
    - x = pixels in 82x36 image patches
    - y = 1 or 0
    - collect large training data sets of positive and negative examples, can be 10000s
    - train a NN or other learning algorithm, to take as input the 82x36 images, and 
      classify those images as y = 1 or 0

Say we get a test set image and we want to find the pedestrians in the image
    - first, take a rectangular 82x36 patch in the image like in the top left corner
    - then run the image patch through your classifier, hopefully returning y = 0
    - next, we slide the window a bit to the right, then run that patch through the classifier
    
The amount you should slide the window is a parameter called the step-size or stride. 
If you step it over 1 pixel at a time, it generally performs best, but moving it more pixels like 
4 would let you move the window more each time.

We can continuously keep stepping through the image with our sliding window over the different
locations in the image through your classifier.

Next, we can start looking at larger image patches and run those through the classifier as well.
Since the classifier only takes 82x36 images, we would resize whatever we captured in our bigger
window and pass that in.

-----

Back to text detection

We can come up with a bunch of positive and negative examples again, where they're all patches
of examples that have or don't have text.

Now we can run a sliding window, this time at one fixed scale. Once you run through the whole image,
the output will be a grayscale image where black is where y=0 and white is where y=1. The axes
are the same. 

Afterwards, we take the black and white image and apply an expansion operator.
    - takes each white region and expands it outwards
        - if a pixel is within 5 pixels of a white pixel, then color it white as well (BFS)
    - we can use a simple heuristic to rule out white boxes that look funny, like thin tall blobs
      or random white boxes by themselves

-----

Character segmentation - 1D sliding window

Use a supervised learning algorithm with a set of positive and negative examples
Is there a character split of 2 different characters in the captured image? y=1 if yes, y=0 if no
y=0 includes images that have one whole character.

Use a sliding window that goes through the text images you got from text detection and get 
your images to put into the classifier.

-----

Photo OCR pipeline
    - text detection
    - character segmentation
    - character classification - similar to multiclass classification problem like from number detection

==================================================

Getting Lots of Data and Artificial Data

Where do you get so much training data from? 
Artificial data synthesis is an easy way to get a huge training set to give to your learning algorithm.

There are 2 variations: 
    - creating data from scratch
    - if we already have a small labeled training set, and we somehow amplify that training set

-----

Character recognition from the OCR pipeline

If we go out and collect a large labeled data set of square image patches, we want the classifier to
recognize the character in the middle of the image patch.

Modern computers often have a large font library, and some word processors have a bunch of fonts as well.
The internet has many free font libraries as well.

If you want more training examples, you can take the characters from those fonts and paste them into
a random background, and now you have new training examples of those characters. You can also apply
some blurring operators, like scaling or rotating or blurring out the letters.

You can create a synthetic set of data that looks remarkably similar to the real data.

-----

Amplifying your current training set with character recognition

You can also take your existing training data set and take a real example from it to amplify your training set.

Take some training example that has an A in it, then distort the A in various ways for your training set.
There are certain distortions to use that may make more sense for different ML applications.

-----

Speech recognition

Say you have audio clips and you want to recognize what words were spoken in the clip.
Original audio: zero one two three four five

Introduce some audio distortions on the example to amplify your set:
    - audio on bad cellphone connection
    - noisy background: crowd
    - noisy background: machinery

By synthesizing additional distortions and introducing different background sounds, we've multiplied
this one example into many more examples by modifying the clean audio clip.

-----

Distortion introduced should be representative of the type of noise/distortions in the test set.
    - characters: widen, rotations
    - audio: background noise, bad phone connection

Usually does not help to add purely random/meaningless noise to your data.
    - characters: for each pixel, add some random Gaussian noise
    - x_1 = intensity (brightness) of pixel i
    - x_i <- x_i + random noise

-----

Discussion on getting more data

1) Make sure you have a low bias classifier before expending the effort by plotting learning curves.
    - keep increasing the # features/# hidden units in NN until you have a low bias classifier
2) "How much work would it be to get 10x as much data as we currently have?"
    - a good question for the product team, and it can usually be done in a few days
    - can be done with artificial data synthesis and collecting/labeling it yourself
        - maybe it takes me 10 seconds to label one training example, and we currently have 1000 examples,
          how much time would I need to obtain and label 10000 examples? (3.5 days)
        - this can be a great way to get much better performance
    - crowdsourcing (e.g. Amazon Mechanical Turk)
        - there are a few websites and services that let you hire people on the web to inexpensively
          label large training sets for you

==================================================

Ceiling Analysis: What Part of the Pipeline to Work on Next

As a ML dev, one of the most valuable resources is your time, so you want to avoid wasting it by
spending too much time on one component, or spending months on a component that doesn't make a huge
difference on the performance of the final system. 

Ceiling analysis can give you a strong guidance on what parts of the pipeline might be the best use
of your time to work on.

-----

Photo ORC pipeline example

Image -> text detection -> character segmentation -> character recognition

What part of the pipeline should you spend the most time trying to improve? Where should you allocate your 
resources?
Each component could be built by a small team of engineers, or the whole system could be built by one person.

Ceiling analysis:
    - it would be nice to have a single rolled number evaluation metric for this learning system
    - say we pick character level accuracy, so if you're given a set test image, what is the
      fraction of characters in a test set image that we recognize correctly?
    - we find that the overall system has 72% accuracy after running it through every module

    - we will go through our first module, text detection, and monkey around with the test set
        - for every test example, we will provide it the correct text detection outputs, so we will simulate
          text detection with 100% accuracy manually
        - then use these ground truth data as our test data set for the next module in the pipeline, to 
          ensure a fully correct test set for it
        - say text detection is now 89%
    - then go to character segmentation, and use our current test data set as input
        - again, we will need to go to the test set and give it the 100% accurate text detection output
          and the correct character segmentation output to compare against
        - character segmentation goes up to 90%
    - finally, go through the character recognition with your perfect character segmentation test data set, 
      and we have 100% accuracy.

We can now understand the upside potential of improving each component:
    - if we get perfect text detection, our performance went up 17%
        - if we spent a lot of time improving it, we can improve the performance by 17%,
          so it's well worth our while
    - perfect character segmentation only went up 1%, so it won't make much impact if we kept working on it
    - perfect character recognition went up 10%, so it may or may not be worth your while

-----

Ceiling analysis example from face recognition from images (artificial example)

Camera image -> preprocess (remove background) -> face detection -> eyes segmentation
                                                                 \                    \
                                                                  > nose segmentation  -> logistic regression -> label
                                                                 \                    /
                                                                  > mouth

Component           |   Accuracy
Overall system            85%
Preprocess               85.1%      ^ 0.1%, so perfect background removal shouldn't really be worked on
Face detection            91%       ^ 5.9%, so it's a big jump that we can put more effort in
Eyes segmentation         95%       ^ 4%, also worth our while to improve
Nose segmentation         96%       ^ 1%
Mouth segmentation        97%       ^ 1%
Logistic regression      100%       ^ 3%, might be worth working on

Performance should in general be going up.

Don't be going with your gut feeling, do some ceiling analysis to see what you should put more work in!