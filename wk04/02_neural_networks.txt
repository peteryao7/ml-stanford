Model Representation I

Neural networks were developed as simulating networks of neurons in the brain.
The brain is packed full of cells, or neurons. They have a cell body and a 
number of input wires, or dendrites. They receive inputs from other locations.
The axon is the output wire used to send signals/messages to other neurons.

Essentially, it's a computational unit that takes in a number of inputs through
its input wires, does some computation on it, then outputs via its axon to other
nodes/neurons in the brain. 

Neurons communicate with spikes, or pulses of electricity, through its axon. The
axon connects to the dendrites of other neurons and uses the output as their input.
Our senses and muscle movement work the same way.

-----

Neuron model: Logistic unit

x_1 
    \
x_2 - O -> h_θ(x)   where h_θ(x) = 1/(1+e^(-θTx))
    /
x_3

Sometimes there could be an x_0 unit, known as the bias unit, but typically x_0 = 1.

Sigmoid (logistic) activation function: g(z) = 1/(1+e^-z)
Weights: parameters (θ)

-----

Neural network - a group of different neurons strung together. 

Inputs (input layer): x_1, x_2, x_3
Neurons (hidden layer): a_1^(2), a_2^(2), a_3^(2)
Strung together to a node (output layer) to form output h_Θ(x)

There can be more than one hidden layer, they're any layer that doesn't represent the 
input or output layers.

a_i^(j) = "activation" of unit i in layer j, starts at layer 2 (1 = input)
Θ^(j) = matrix of weights controlling function mapping from layer j to layer j+1

        Note the capital theta.

Hidden units:
a_1^(2) = g(Θ_1,0^(1)*x_0 + Θ_1,1^(1)*x_1 + Θ_1,2^(1)*x_2 + Θ_1,3^(1)*x_3)
a_2^(2) = g(Θ_2,0^(1)*x_0 + Θ_2,1^(1)*x_1 + Θ_2,2^(1)*x_2 + Θ_2,3^(1)*x_3)
a_3^(2) = g(Θ_3,0^(1)*x_0 + Θ_3,1^(1)*x_1 + Θ_3,2^(1)*x_2 + Θ_3,3^(1)*x_3)

Output unit: 
h_Θ(x) = a_1^(3) = g(Θ_1,0^(2)*a_0^(2) + Θ_1,1^(2)*a_1^(2) + Θ_1,2^(2)*a_2^(2) + Θ_1,3^(2)*a_3^(2))

Θ^(1) ∈ R^(3x4) matrix

If a network has s_j untis in layer j, s_(j+1) units in layer j+1, then Θ^(j) will be of dimension 
s_(j+1) x ((s_j) + 1).

    For example, if layer 1 (s_1) has 2 units and layer 2 (s_2) has 4 units, then Θ^(1) has dimension
    4 x 3.

----------

Model Representation II

Consider the previous neural network from before.

z_1^(2) = Θ_1,0^(1)*x_0 + Θ_1,1^(1)*x_1 + Θ_1,2^(1)*x_2 + Θ_1,3^(1)*x_3
so a_1^(2) = g(z_1^(2))

Similarly,
a_2^(2) = g(z_2^(2))
a_3^(2) = g(z_3^(2))

We can vectorize the computation of this neural network as so:

x = [x_0]       z^(2) = [z_1^(2)]
    [x_1]               [z_2^(2)]
    [x_2]               [z_3^(2)]
    [x_3]

z^(2) = Θ^(1)*x, where x = a^(1)
a^(2) = g(z^(2))

Now we have a_1^(2), a_2^(2), a_3^(3). We still need one more value, a_0^(2), a bias unit in the 
hidden layer going to the output layer. We can just add a_0^(2) = 1, so a^(2) ∈ R^4

Thus,
z^(3) = Θ^(2)*a^(2)
h_Θ(x) = a^(3) = g(z^(3)), which is the output layer.

This process of computing h_Θ(x) is called forward propagation.

-----

Neural Network learning its own features

If we only have 2 layers, the input and output layers, it pretty much looks like logistic regression, 
but rather using the original features x_1, x_2, x_3, it's using the features a_1^(2), a_2^(2), a_3^(2),
which are learned as functions of the input. 

If we introduce new layers, we can think of the input layer as learning its own features to feed
into the logistic regression instead of using the raw features.

Consider the network:

x_1     O
                O
x_2     O               O
                O
x_3     O

Let a^(1) = x ∈ R^(n+1) and denote the input with a_0^(1) = 1.
To compute a^(2):
z^(2) = Θ^(1)a^(1)
a^(2) = g(z^(2))

-----

General case:

Define a new variable z_k^(j) that encompasses the parameters inside g.

a_1^(2) = g(z_1^(2))
a_2^(2) = g(z_2^(2))
a_3^(2) = g(z_3^(2))

In other words, for layer j=2 and node k, z will be:
z_k^(2) = Θ_k,0^(1)*x_0 + Θ_k,1^(1)*x_1 + ... + Θ_k,n^(1)*x_n

where the vector representations of x and z^j are:
x = [x_0]   z^(j) = [z_1^(j)]
    [x_1]           [z_2^(j)]
    [...]           [  ...  ]
    [x_n]           [z_n^(j)]

Setting x = a^(1), the equation can be written as:
z^(j) = Θ^(j-1)*a(j-1)
where Θ^(j-1) has dimensions s_j x (n+1) and a^(j-1) with height n+1.

Then we can calculate:
a^(j) = g(z^(j))

And finally:
h_Θ(x) = a^(j+1) = g(z^(j+1))