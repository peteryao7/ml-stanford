Deciding What to Try Next

How to decide which algorithm to apply to certain ML problems

-----

Debugging a learning algorithm:

$ you have implemented regularized linear regression to predict housing prices.

J(θ) = (1 / (2*m)) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2 + λ*sum(j=1, m) θ_j^2)

However, when you test your hypothesis on a new set of houses, you find that
it makes unacceptably large errors in its predictions. What should you try next?
    - get more training examples
    - try smaller sets of features
    - try getting additional features
    - try adding polynomial features (x_1^2, x_2^2, x_1*x_2 etc.)
    - increasing or decreasing λ

Commonly, people just pick to do what their gut feeling is and waste time trying
an option that doesn't make sense. 

There's a simple technique that rules out many of those options and saves time.

-----

Machine learning diagnostic:

A diagnostic is a test that you can run to gain insight of what is/isn't working
with a learning algorithm, and gain guidance as to how best to improve its
performance.

Diagnostics can take time to implement, but can save a lot of time in the long run.

==================================================

Evaluating a Hypothesis

When we fit the parameters of our learning algorithm, we think about choosing params
that minimize the training error. However, getting a low training error introduces
the risk of overfitting our data, which is terrible for generalizing new data.

One thing we can do is simply plot the data, but what about data with a ton of
features? 

Suppose we have a data set:

size    price
2104    400     
1600    330
2400    369
1416    232     Use these 7 examples for our training set
3000    540
1985    300
1534    315
-------------
1427    199
1380    212     Use these 3 examples for our test set
1494    243

We can split the data up 70/30 for our new training/test sets.

Now we have in our training set ((x^(1),y^(1)), ..., (x^(m),y^(m))) and 
(x_test^(1),y_test^(1), ..., (x_test^(m_test),y_test^(m_test))), where
m_test is the # of test examples.

If your data is ordered, then you should randomly shuffle your training set, then
pick the first 70% of data in your training set, otherwise you can pick the first
70% without shuffling.

-----

Training/testing procedure for linear regression

Learn parameter θ from training data, minimizing error J(θ)
Compute test set error:
J_test(θ) = (1 / (2*m_test)) * sum(i=1, m_test) (h_θ(x_test^(i)) - y_test^(i))^2

-----

Training/testing procedure for logistic regression

Learn parameter θ from training data
Compute test set error:
J_test(θ) = (-1 / m_test) * sum(i=1, m_test) (y_test^(i) * log(h_θ(x_test^(i))) + (1-y_test^(i)) * log(h_θ(x_test^(i))))

Sometimes there's an alternative test set metric that's easier to interpret, the misclassification error:
err(h_θ(x), y) = 1 if h_θ(x) >= 0.5, y = 0
                   or h_θ(x) < 0.5, y = 1
                 0 otherwise

This gives us a binary 0 or 1 error result based on a misclassification. 
The average test error for the test set is:

Test error = 1 / m_test * sum(i=1, m_test) err(h_θ(x_test^(i)), y_test^(i))

which gives us the proportion of the test data that was misclassified.

==================================================

Model Selection and Train/Validation/Test Sets

Once parameters θ_0, θ_1, ..., θ_n were fit to some set of data (training set), the error
of the parameters as measured on that data (the training error J(θ)) is likely to be lower than
the actual generalization error.

-----

Model Selection
d=1) h_θ(x) = θ_0 + θ_1x                                    => θ^(1) => J_test(θ^(1))
d=2) h_θ(x) = θ_0 + θ_1x + θ_2x^2                           => θ^(2) => J_test(θ^(2))
d=3) h_θ(x) = θ_0 + θ_1x + θ_2x^2 + θ_3x^3                  => θ^(3) => J_test(θ^(3))
...
d=10) h_θ(x) = θ_0 + θ_1x + θ_2x^2 + ... + θ_10x^10         => θ^(10) => J_test(θ^(10))

$ we chose θ_0 + ... + θ_5x^5
How well does the model generalize? Report test set error J_test(θ^(5)).
Problem: J_test(θ^(5)) is likely to be an optimistic estimate of generalization error, so it 
may not do as well on new data compared to our current test set.
I.e. our extra parameter (d = deg of polynomial) is fit to the test set.

-----

Evaluating your hypothesis

Splitting our dataset into 3 pieces:

size    price
2104    400     
1600    330
2400    369
1416    232     Use these 6 examples for our training set
3000    540
1985    300
-------------
1534    315     Use these 2 examples for our cross validation (CV) set
1427    199
-------------
1380    212     Use these 2 examples for our test set
1494    243

We can split the data up 60/20/20 for our new training/CV/test sets.
Now with our training and test set, we have ((x_cv^(1),y_cv^(1)), ..., (x_cv^(m_cv),y_cv^(m_cv)))
for our CV set.

The CV error is also similar:
J_cv(θ) = (1/2m_cv) * sum(i=1, m_cv) (h_θ(x_cv^(i)) - y_cv^(i))^2

-----

Now we can select our model from before based on the lowest CV error.

Say J_cv(θ^(4)) gave us the lowest CV error, so we would pick:
θ_0 + θ_1x_1 + ... + θ_4x^4
for our model, and we avoid training the degree of the polynomial d with the test set.