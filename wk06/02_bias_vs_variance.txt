Diagnosing Bias vs. Variance

If your learning algorithm isn't running as well as you're hoping, it's because
you have either a high bias or high variance problem, in other words, an
underfitting or overfitting problem. Distinguishing which one it is is critical in 
improving your algorithm.

If d is too low, it can underfit, and if d is too high, it can overfit.

Training error: J_train(θ) = (1 / (2*m)) sum(i=1, m) (h_θ(x(i)) - y^(i))^2
CV error (or J_test(θ)): J_cv(θ) = (1/(2*m_cv)) * sum(i=1, m_cv) (h_θ(x_cv^(i)) - y_cv^(i))^2

If we graphed the degree of polynomial d against the error, we can see that:
    - J_train(θ) will produce a concave curve in behavior, approaching 0 as d -> inf
    - J_cv(θ) will produce an upward-facing parabolic curve in behavior,
      somewhere around the vertex is where the optimal value of d will be

-----

Suppose your learning algorithm is performing less well than you were hoping (J_cv(θ) or J_test(θ) is high).
Is it a bias or variance problem?

If you're fitting a low degree polynomial, it's a bias (underfitting) problem.
    - J_cv(θ) and J_train(θ) are both high
    - J_cv(θ) ≈ J_train(θ), cv possibly higher
If you're fitting a high degree polynomial, it's a variance (overfitting) problem.
    - J_cv(θ) is high, J_train(θ) is low
    - J_cv(θ) >> J_train(θ) (much greater than)

E.g. if J_train(θ) = 0.10 and J_cv(θ) = 0.30, then you have a high variance (overfitting) problem.

==================================================

Regularization and Bias/Variance

Regularization can help prevent overfitting, but how does it influence bias/variance?

-----

Suppose our model is h_θ(x) = θ_0 + θ_1x + θ_2x^2 + θ_3x^3 + θ_4x^4 and
J(θ) = 1/(2m) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2) + λ/(2m) * sum(j=1, n) θ_j^2

Using a large λ for our J(θ), then we get high bias. Conversely, with a small λ, we get high variance.

For J_train(θ), the regularization is the sum of squared errors on the training set w/o taking
regularization account. The same goes for J_cv(θ) and J_test(θ).

1) Try λ = 0        => min θ J(θ) => Θ^(1) => J_cv(Θ^(1))
2) Try λ = 0.01     => min θ J(θ) => Θ^(2) => J_cv(Θ^(2))
3) Try λ = 0.02     => min θ J(θ) => Θ^(3) => J_cv(Θ^(3))
4) Try λ = 0.04     => min θ J(θ) => Θ^(4) => J_cv(Θ^(4))
5) Try λ = 0.08     => min θ J(θ) => Θ^(5) => J_cv(Θ^(5)) => pick this assuming this is the min
...
12) Try λ = 10.24   => min θ J(θ) => Θ^(12) =>  J_cv(Θ^(12))

Now we have 12 possible values of λ to check. Say we pick Θ^(5), then we find the test error J_test(Θ^(5)).

J(θ) = 1/(2m) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2) + λ/(2m) * sum(j=1, n) θ_j^2
J_train(θ) = (1/2*m) sum(i=1, m) (h_θ(x^(i)) - y^(i))^2
J_cv(θ) = (1/2*m_cv) sum(i=1, m_cv) (h_θ(x_cv^(i)) - y_cv^(i))^2

For small λ, you have a high variance problem, and for large λ, you have a high bias problem.
    - J_train(λ) will increase as λ increases
    - J_cv(λ) will have a parabolic behavior, where it's high when λ is too low and too high
    - J_cv(λ) > J_train(λ)