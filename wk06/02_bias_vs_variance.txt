Diagnosing Bias vs. Variance

If your learning algorithm isn't running as well as you're hoping, it's because
you have either a high bias or high variance problem, in other words, an
underfitting or overfitting problem. Distinguishing which one it is is critical in 
improving your algorithm.

If d is too low, it can underfit, and if d is too high, it can overfit.

Training error: J_train(θ) = (1 / (2*m)) sum(i=1, m) (h_θ(x(i)) - y^(i))^2
CV error (or J_test(θ)): J_cv(θ) = (1/(2*m_cv)) * sum(i=1, m_cv) (h_θ(x_cv^(i)) - y_cv^(i))^2

If we graphed the degree of polynomial d against the error, we can see that:
    - J_train(θ) will produce a concave curve in behavior, approaching 0 as d -> inf
    - J_cv(θ) will produce an upward-facing parabolic curve in behavior,
      somewhere around the vertex is where the optimal value of d will be

-----

Suppose your learning algorithm is performing less well than you were hoping (J_cv(θ) or J_test(θ) is high).
Is it a bias or variance problem?

If you're fitting a low degree polynomial, it's a bias (underfitting) problem.
    - J_cv(θ) and J_train(θ) are both high
    - J_cv(θ) ≈ J_train(θ), cv possibly higher
If you're fitting a high degree polynomial, it's a variance (overfitting) problem.
    - J_cv(θ) is high, J_train(θ) is low
    - J_cv(θ) >> J_train(θ) (much greater than)

E.g. if J_train(θ) = 0.10 and J_cv(θ) = 0.30, then you have a high variance (overfitting) problem.

==================================================

Regularization and Bias/Variance

Regularization can help prevent overfitting, but how does it influence bias/variance?

-----

Suppose our model is h_θ(x) = θ_0 + θ_1x + θ_2x^2 + θ_3x^3 + θ_4x^4 and
J(θ) = 1/(2m) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2) + λ/(2m) * sum(j=1, n) θ_j^2

Using a large λ for our J(θ), then we get high bias. Conversely, with a small λ, we get high variance.

For J_train(θ), the regularization is the sum of squared errors on the training set w/o taking
regularization account. The same goes for J_cv(θ) and J_test(θ).

1) Try λ = 0        => min θ J(θ) => Θ^(1) => J_cv(Θ^(1))
2) Try λ = 0.01     => min θ J(θ) => Θ^(2) => J_cv(Θ^(2))
3) Try λ = 0.02     => min θ J(θ) => Θ^(3) => J_cv(Θ^(3))
4) Try λ = 0.04     => min θ J(θ) => Θ^(4) => J_cv(Θ^(4))
5) Try λ = 0.08     => min θ J(θ) => Θ^(5) => J_cv(Θ^(5)) => pick this assuming this is the min
...
12) Try λ = 10.24   => min θ J(θ) => Θ^(12) =>  J_cv(Θ^(12))

Now we have 12 possible values of λ to check. Say we pick Θ^(5), then we find the test error J_test(Θ^(5)).

J(θ) = 1/(2m) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2) + λ/(2m) * sum(j=1, n) θ_j^2
J_train(θ) = (1/2*m) sum(i=1, m) (h_θ(x^(i)) - y^(i))^2
J_cv(θ) = (1/2*m_cv) sum(i=1, m_cv) (h_θ(x_cv^(i)) - y_cv^(i))^2

For small λ, you have a high variance problem, and for large λ, you have a high bias problem.
    - J_train(λ) will increase as λ increases
    - J_cv(λ) will have a parabolic behavior, where it's high when λ is too low and too high
    - J_cv(λ) > J_train(λ)

==================================================

Learning Curves

Learning curves are useful for sanity checking if your algorithm is working correctly, 
or if you want to improve its performance. They're also useful for diagnosing an algorithm
for bias and/or variance.

To plot a learning curve, plot J_train(θ) or J_cv(θ) as a function of m, the number of
training examples you have. Deliberately limit the number of training examples used to
around 10-40 and plot what the training and cross validation are for the small set
of training exercises.

Say for h_θ(x) = θ_0 + θ_1x + θ_2x^2
    - for m=1, we can fit it perfectly with many quadratic functions
    - for m=2, we can still fit it quite well
    - for m=3, we could still fit it perfectly

For m = 1, 2, 3 the training error will be 0 assuming regularization isn't used, or slightly
0 if it is used. 
If the training set is small, then the error is likely to also be small.

    - for m=4, we might not be able to fit all the data perfectly

As m grows, it gets harder and harder to map a function that crosses every single training example.
That means as m grows, then the error grows.

Conversely as m increases, J_cv(θ) will start high, then decrease.

-----

High Bias

For models that underfit the data:
    - J_cv(θ) will start high, decreases, and start to plateau out
    - J_train(θ) will start low, increases, and J_train(θ) ≈ J_cv(θ)
    - J_cv(θ) and J_train(θ) will both be quite high, indicating high error

Getting more training data by itself won't help all that much. 

-----

High Variance

For models that overfit the data:
    - J_cv(θ) will start high, decreases, and plateaus a bit
    - J_train(θ) will start low, increases, and stay fairly low
    - there's a huge gap between J_cv(θ) and J_train(θ)

Getting more training data is likely to help.
If we extended the graph to include more of m, J_cv(θ) and J_train(θ) will 
start getting closer to each other.