Prioritizing What to Work On - ML System Design for a Spam Classifier

Two examples of spam vs. non-spam emails

Spam emails have:
    - suspicious e-mails and subjects
    - keywords for selling things (deal, buy)
    - misspelled words

vs. a non-spam email, like an email from a family member

Labels y=1 for spam and y=0 for non-spam.

-----

Supervised learning for building a classifier to distinguish spam vs. non-spam

x = features of email
y = spam (1) or not spam (0)

Features x: Choose 100 words indicative of spam/not spam
    - deal, buy, discount, andrew, now, ...

Given a spam mail, we can take the list of words, sort them in lexicographical order,
and check if these words appear in the e-mail or not.

Then define a feature vector x of size 100x1 and if a word appears, set the index as 1,
if not, set as 0.

-----

From: cheapsales@buystufffromme.com
To: ang@cs.standord.edu
Subject: Buy now!

Deal of the week! Buy now!

x = [ 0 ]     andrew
    [ 1 ]     buy
    [ 1 ]     deal
    [ 0 ]     discount
    [...]
    [ 1 ]     now

In practice, we'd take the most frequently occurring n words (10000 or 50000) in training
set, rather than manually picking 100 words.

-----

How to spend your time to make it have low error?
    - collect lots of data
        - e.g. a "honeypot" project creates fake e-mail addresses to send to spammers
          for them to use, so the creators get tons of spam email to use for data to train
          their learning algorithms
    - develop sophisticated features based on email routing information (from email header)
        - when spammers send e-mail, they'll try to obscure the origins of the e-mail, or
          use fake e-mail headers, or send the e-mail through unusual routes
        - some of this info is reflected in the e-mail header
    - develop sophisticated features for the message body
        - should "discount" or "discounts" be treated as the same word?
        - what about "deal" or "Dealer"
        - punctuation?
    - develop sophisticated algorithm to detect misspellings
        - m0rtgage, med1cine, w4tches

It's difficult to see which one is the most helpful.

-----

When working on a ML problem, it's helpful to brainstorm lists of different things to try
like above. Avoid focusing on one single problem or going with a gut feeling. 

==================================================

Error Analysis

Recommended approach:
    - start with a simple algorithm that you can implement quickly
        - implement it and test it on your cross-validation data
    - plot learning curves to decide if more data, more features etc. are likely to help
        - there's not much of a way to see in advance whether your algorithm suffers from
          high bias or high variance, and you can use that to decide if you want more
          data, more features etc.
    - error analysis: manually examine the examples in cross-validation set that your
      algorithm made errors on
        - see if you can spot any systematic trend in what type of examples it is making
          errors on
        - e.g. see which emails are being misclassified as spam/not spam, then try finding
          any patterns you can use to design new features

-----

Let's say you have 500 examples in your CV set (m_cv = 500)
Algorithm misclassifies 100 e-mails.
Manually examine the 100 errors and categorize them based on:
    - what type of email it is (pharmacy, replicas, phishing, ...)
    - what cues (features) you think would have helped the algorithm classify them correctly

Pharma: 12
Replica/fake: 4
Phishing: 53     <= the algorithm is doing particularly poorly for these e-mails
Other: 31

Deliberate misspellings: 5      <= sufficiently rare phenomenon, may not be worth focusing on
Unusual e-mail routing: 16 
Unusual punctuation: 32         <= there's a lot, maybe it's more worth your while

This kind of error analysis, where you manually examine the mistakes of the algorithm, can
guide you to the most fruitful avenues to pursue. And the fastest way to get these results
and identify the hard examples is with a quick and dirty implementation of the algorithm.

-----

The importance of numerical evaluation

You want your algorithm to return a single real number, like accuracy or error, that tells
you how well it's doing easily. One candidate is the CV error.

Should discount/discounts/discounted/discounting be treated as the same word?
    - can use "stemming" software (e.g. "Porter stemmer") to treat these as the same word
    - but watch out for words that look similar but are unrelated (universe/university)

Error analysis may not be helpful for deciding if this is like to improve performance, but
the only solution is to try it and see if it works.

Say we look at the CV error for stemming vs. no stemming:
    - stemming: 3% error
    - non-stemming: 5% error
So using stemming is probably a good idea.

What about distinguishing upper vs. lower case (Mom/mom)?
    - with distinguishing: 3.2% error, which is worse than only stemming
    - since 3.2% > 3%, we shouldn't include it

With a single real number, we only need to see if a number goes up or down to evaluate our
results, instead of changing parts of an algorithm and making comparisons manually every time.