The Problem of Overfitting

Overfitting can cause linear and logistic regression to act poorly.
To fix it, we can use regularization.

Ex. Linear regression for housing prices

"underfit" and "high bias" - the model doesn't even fit the training data
    - the model says the price will vary linearly with the size, but the
      data says the contrary
    - e.g. using a linear function to fit data with a quadratic behavior

"overfit" and "high variance" - the moeel fits the training data "too well"
    - the model could fit all the current data, but is bad at generalizing for new data
    - almost as if the data could fit any function and is too variable
    - e.g. using an order 5 polynomial to fit data with a quadratic behavior

"generalize" - how a model can correctly predict new data outside the training data

Ex. Logistic regression

"underfit" - using a straight line to separate positive/negative examples with
             quadratic behavior

"overfit" - using an order 6 polynomial that contorts itself to make a boundary that
            goes to great lengths to fit every single training datum

-----

Addressing overfitting:

Plotting the hypothesis and assessing the behavior yourself isn't always the best way.

More often than not, we might have learning problems where we have a bunch of features,
like size of house, # bedrooms, age of house etc., and it's hard to decide which
features to keep or not when we don't have much fitting data.

Options:
    - reduce number of features
        - manually select which features are most important to keep and which to throw out
        - model selection algorithm (later in course)
    - regularization
        - keep all features, but reduce magnitude/values of parameters θ_j
        - works well when we have a lot of features, each of which contributes a bit to predicting y

----------

Cost Function for Regularization

Let the function θ_0 + θ_1x + θ_2x^2 fit the data well
and θ_0 + θ_1x + θ_2x^2 + θ_3x^3 + θ_4x^4 overfit the data

Suppose we penalize θ_3 and θ_4 and make them really small:

min_θ 1/(2m) * (sum (i=1, m) (h_θ(x^(i)) - y^(i))^2) + 1000θ_3^2 + 1000θ_4x^2
where θ_3 and θ_4 are very close to 0.

Small values for parameters θ_0, θ_1, ..., θ_n
    - "simpler" hypothesis
    - less prone to overfitting

Ex. Housing:
    - features: x_1, x_2, ..., x_100
    - parameters: θ_0, θ_1, ..., θ_100

We have 101 parameters and we don't know which parameters that we should shrink.
What we can do is take our cost function for linear regeression and modify it to
shrink all the parameters, then add an extra regularization term at the end:




----------

Regularized Linear Regression

----------

Regularized Logistic Regression