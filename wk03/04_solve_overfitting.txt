The Problem of Overfitting

Overfitting can cause linear and logistic regression to act poorly.
To fix it, we can use regularization.

Ex. Linear regression for housing prices

"underfit" and "high bias" - the model doesn't even fit the training data
    - the model says the price will vary linearly with the size, but the
      data says the contrary
    - e.g. using a linear function to fit data with a quadratic behavior

"overfit" and "high variance" - the moeel fits the training data "too well"
    - the model could fit all the current data, but is bad at generalizing for new data
    - almost as if the data could fit any function and is too variable
    - e.g. using an order 5 polynomial to fit data with a quadratic behavior

"generalize" - how a model can correctly predict new data outside the training data

Ex. Logistic regression

"underfit" - using a straight line to separate positive/negative examples with
             quadratic behavior

"overfit" - using an order 6 polynomial that contorts itself to make a boundary that
            goes to great lengths to fit every single training datum

-----

Addressing overfitting:

Plotting the hypothesis and assessing the behavior yourself isn't always the best way.

More often than not, we might have learning problems where we have a bunch of features,
like size of house, # bedrooms, age of house etc., and it's hard to decide which
features to keep or not when we don't have much fitting data.

Options:
    - reduce number of features
        - manually select which features are most important to keep and which to throw out
        - model selection algorithm (later in course)
    - regularization
        - keep all features, but reduce magnitude/values of parameters θ_j
        - works well when we have a lot of features, each of which contributes a bit to predicting y

----------

Cost Function for Regularization

Let the function θ_0 + θ_1x + θ_2x^2 fit the data well
and θ_0 + θ_1x + θ_2x^2 + θ_3x^3 + θ_4x^4 overfit the data

Suppose we penalize θ_3 and θ_4 and make them really small:

min_θ 1/(2m) * (sum (i=1, m) (h_θ(x^(i)) - y^(i))^2) + 1000θ_3^2 + 1000θ_4x^2
where θ_3 and θ_4 are very close to 0.

Small values for parameters θ_0, θ_1, ..., θ_n
    - "simpler" hypothesis
    - less prone to overfitting

Ex. Housing:
    - features: x_1, x_2, ..., x_100
    - parameters: θ_0, θ_1, ..., θ_100

We have 101 parameters and we don't know which parameters that we should shrink.
What we can do is take our cost function for linear regeression and modify it to
shrink all the parameters, then add an extra regularization term at the end of the sum:

λ * (sum(i=1, n) θ_j^2

that will shrink every single parameter. λ is the regularization parameter that
determines how much the costs of the θ parameters are infalted.

Our cost function is now:
J(θ) = 1/(2m) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2) + λ * (sum(i=1, n) θ_j^2)

We can use it to create a much better hypothesis for the data. Although the model
will still have the original order, the behavior of the model is much closer to
what it should be.

However, if the regularization parameter λ is set too large, like 10^10, then
the hypothesis will underfit the data. All the parameters will be close to 0, 
which is like getting rid of all the hypotheses for each feature.

h_θ(x) = θ_0

which is just a horizontal line, prone to underfitting data.

----------

Regularized Linear Regression

J(θ) = (1/2m) * (sum(i=1, m) (h_θ(x^(i)) - y^(i))^2 + λ * sum(j=1, n) θ_j^2)
min θ J(θ)

GD:

repeat {
    θ_0 := θ_0 - α * 1/m * (sum(i=1, m) (h_θ(x^(i)) - y^(i))*x_0^(i))
    θ_j := θ_j - α * (1/m * (sum(i=1, m) (h_θ(x^(i)) - y^(i))*x_j^(i)) + λ/m * θ_j)
}
for j ∈ {1,2,...,n}, not 0]]

Equivalently, θ_j := θ_j(1 - α * λ/m) - α/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_j^(i)

-----

Normal equation

X = [(x^(1))T]          y = [y^(1)]
    [   ...  ]              [ ... ]
    [(x^(m))T]              [y^(m)]

min θ J(θ)

θ = (x^Tx + λ * L)^-1 * X^T * y

where L is a matrix of zeros with 0 at the top left, then 1s down the rest of the diagonal.
L has a dimension (n+1)x(n+1)

If m < n, then X^TX is non-invertible (singular), but when we add the term λ*L, 
then X^TX + λ*L is invertible.

----------

Regularized Logistic Regression

Using GD and the more advanced optimization techniques for regularized logistic regression.

All we need to do is append the regularization to our logistic regression cost function:

J(θ) = -(1/m) * (sum(i=1, m) (y^(i)*log(h_θ(x^(i))) + (1-y^(i))*log(1-h_θ(x^(i)))))
       + λ/(2m) * sum(j=1, n) θ_j^2

The second term means to explicitly exclude the bias term, θ_0. 

For GD:

Repeat {
    θ_0 := θ_0 - α * (1/m) * (sum(i=1,m) (h_θ(x^(i)) - y^(i)) * x_0^(i)
    θ_j := θ_j - α * (1/m) * (sum(i=1,m) (h_θ(x^(i)) - y^(i)) * x_j^(i) + λ/m * θ_j^2) 
}

where 1/m * sum is the derivative of the cost function with respect to θ_j 
and h_θ(x) = 1/(1+e^(-θ^Tx))

-----

Advanced optimization:

function [jVal, gradient] = costFunction(theta)
    jVal = code to compute J(θ);
    J(θ) = -(1/m) * sum(i=1, m) (y^(i)*log(h_θ(x^(i))) + (1-y^(i))*log(1) - h_θ(x^(i))) + λ/(2m) * sum(j=1, n) θ_j^2
    gradient(1) = code to compute ∂/(∂θ_0) J(θ);
    1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_0^(i)
    gradient(2) = code to compute ∂/(∂θ_1) J(θ);
    1/m * (sum(i=1, m) (h_θ(x^(i)) - y^(i))) + λ/m * θ_1
    gradient(3) = code to compute ∂/(∂θ_2) J(θ);
    1/m * (sum(i=1, m) (h_θ(x^(i)) - y^(i))) + λ/m * θ_2
    ...
    gradient(n+1) = code to compute ∂/(∂θ_n) J(θ);

Remember that the regularization is done *after* the summation, not within it.