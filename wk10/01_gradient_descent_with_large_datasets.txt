Learning with Large Datasets

One reason why learning algorithms work much better now than 5 or 10 years ago is
the sheer amount of data we have now that we can train our algorithms on.

-----

Why do we want to use such large data sets?

One of the best ways to get a high performance ML system is to take a low bias
learning algorithm, and train that on a lot of data, such as classifying between
confusable words, like to/two/too and then/than.

"It's not who has the best algorithm that wins. It's who has the most data."

-----

Learning with large datasets can also lead to computational problems

Say m = 100,000,000 (achievable through datasets like the US census or traffic in popular websites)

If we want to use gradient descent as before:
    θ_j := θ_j - α * 1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_j^(i)
We need to carry out a summation over a hundred million terms to perform a single step of descent.

Maybe we can randomly pick a subset of m = 1000 and train on that as a sanity check, and that may
do just as well.
    - It will do just as well if you plot the learning curves and if your training objective and CV
      objective look like they have high variance (big gap). That signifies high bias, which we know can be
      resolved with more data.
    - If you tried to plot the learning curve for m = 100,000,000 and it looks like you have high variance,
      that also tells you that getting more data than you already have won't be useful.

==================================================

Stochastic Gradient Descent

For many learning algorithms like linear regression, logistic regression, and NN, the way we derived
the algorithms was by coming up with a cost function or optimization objective, and minimizing it
with an algorithm like gradient descent. With a large dataset, gradient descent becomes very slow.

Stochastic GD is a modification of GD that lets us scale these algorithms to much bigger training sets.

-----

Linear regression with GD

h_θ(x) = sum(j=0, n) θ_j*x_j
J_train(θ) = 1/(2m) * sum(i=1, m) (h_θ(x^(i) - y^(i))^2)

repeat {
    θ_j := θ_j - α * 1/(m) * sum(i=1, m) (h_θ(x^(i) - y^(i))^2)
    % 1/m * the summation is also known as ∂/∂θ_j J_train(θ)
    for all j = 0,...,n
}

If n is large like 300 million, then the summation in GD becomes expensive.
This GD is also known as batch GD, which looks at all training examples at a time.

-----

Stochastic GD

cost(θ,(x^(i),y^(i))) = 1/2 * (h_θ(x^(i)) - y^(i))^2
J_train(θ) = 1/m * sum(i=1, m) cost(θ,(x^(i),y^(i)))

1) Randomly shuffle data set (m training examples) as preprocessing step
2) Repeat {
    for i = 1:m
        θ_j := θ_j - α * (h_θ(x^(i)) - y^(i)) * x_j^(i) 
        % (h_θ(x^(i)) - y^(i)) * x_j^(i) = ∂/∂θ_j cost(θ,(x^(i),y^(i)))
        for j = 0,...,n
    end
}

It'll look at the first training example, (x^(1),y^(1)), modify the parameters a bit to fit the 
first training example a little better. In the for loop, it'll move to the second training example,
then modify the parameters again to fit that training example a bit better, then so on until it
reaches the entire training set.

Randomizing the data set will speed up stochastic GD a bit, just in case our data came sorted.

Unlike batch GD, where we need to go through every m to make changes to our parameters, we do it
each training example at a time.

-----

What stochastic GD does to the parameters

Each iteration simply tries to fit the current training example better, so at first, it's possible
that GD heads in the wrong direction. But as we keep iterating on more training examples, the
algorithm will generally move the parameters in the right direction, but not always.

It ends up wandering around continuously in a region close to the global minimum; it may not
even get there exactly, but that's actually ok as it'll be close and that's still a good 
hypothesis nonetheless.

Depending on the size of the training set, doing the loop in step 2 may be done 1-10 times. For
a massive training set with 300 million examples, it's possible that after a single pass through
the loop, you may already have a perfectly good hypothesis. 

In contrast to batch GD, after going through 300 million examples, you'd be making 1 baby step
because you only went through one iteration.

==================================================

Mini-Batch Gradient Descent

Can work even faster than stochastic GD.

Batch GD: use all m examples in each iteration
Stochastic GD: use 1 example in each iteration
Mini-batch GD: use b examples in each iteration, where b = mini-batch size, like 10 (2-100)

So mini-batch GD is like an in-between for batch and stochastic GD.

Get b = 10 examples from training set (x^(i),y^(i)),...(x^(i+9),y^(i+9))
θ_j := θ_j - α * 1/10 * sum(k=i, i+9) (h_θ(x^(k)) - y^(k)) * x_j^(k); % b = 10
i := i + 10;

-----

Algorithm

Say b = 10, m = 1000
Repeat {
    for i = 1, 11, 21, 31, ..., 991 {
        θ_j := θ_j - α * 1/10 * sum(k=i, i+9) (h_θ(x^(k)) - y^(k)) * x_j^(k); for every j = 0,...,n
    }
}

So for m = 300 million, we only need to look at b = 10 first, then maybe the next 10, then the next 10 etc.

-----

Stochastic GD vs. mini-batch GD

Mini-batch is likely to outperform stochastic GD if you have a good vectorized implementation.
For mini-batch GD, the sum over 10 examples can be performed in a more vectorized way, which allows you to
partially parallelize your computation over the 10 examples using the good linalg libraries.

Sometimes, there isn't much to parallelize over from one example in stochastic GD.

One disadvantage for mini-batch GD is that there's an extra parameter b, which may take time to fiddle with.
But with a good vectorized implementation, it may still be worth your time.

==================================================

Stochastic GD Convergence

When you're running stochastic GD, how do you make sure it's completely debugged and is converging ok?
How do you pick a good learning rate α?

-----

Checking for convergence

Batch GD:
    Plot J_train(θ) as a function of the number of iterations of GD
    J_train(θ) = 1/(2*m) * sum(i=1, m) (h_θ(x^(i)) - y^(i))^2

Stochastic GD:
    cost(θ,(x^(i),y^(i))) = 1/2 * (h_θ(x^(i)) - y^(i))^2
    During learning, compute cost(θ,(x^(i),y^(i))) before updating θ using (x^(i),y^(i))
    Every 1000 iterations or so, plot cost(θ,(x^(i),y^(i))) averaged over the last
    1000 examples processed by the algorithm
        - This gives you a running estimate of how well the algorithm is doing on the last 1000
          training examples seen.

In contrast to computing J_train periodically, it doesn't cost much to update cost(θ,(x^(i),y^(i))) in 
comparison and every 1000 iterations or so, we average the last 1000 costs computed and plot that.
Looking at these plots, we can see if stochastic GD is converging.

-----

Plot cost(θ,(x^(i),y^(i))) averaged over the last 1000 or so examples:

Plot 1: wiggly model, but decreases at first and plateaus
    - This is a good sign, and shows that the algorithm has converged
    - Watch out for cases where the algorithm may converge at first, but moves further down as 
      stochastic GD can dance around the global minimum and we can get slightly better results.

You can average over the last 5000 examples instead of 1000 and get a smoother graph. The problem
here is that you're increasing that number drastically, which can affect runtime.

Plot 2: the plot isn't increasing or decreasing
    - The cost isn't increasing or decreasing, so that may be a sign the algorithm isn't learning.
    - If we were to increase to averaging over a larger number of examples, it is possible that 
      the graph decreases a bit, and the original plot was too noisy. 
    - But it is still possible that the graph would still stay flat with 5000 examples, meaning 
      your algorithm isn't learning.

Plot 3: the plot is increasing
    - The algorithm is diverging and the errors are increasing
    - Use a smaller learning rate α
    
-----

In most implementations of stochastic GD, the learning rate α typically stays constant. This is 
why it never converges fully to the global minimum.

If you want it to converge to the global minimum, you can gradually decrease α over time.
    - set α = constant_1 / (iterationNumber + constant_2)
    - iterationNumber is the number of training examples seen so far in stochastic GD, increasing
      over time
    - not recommended, since you need to play around with both constants more and it gets finnicky
    - but if the constants are tuned well, the meanderings get smaller until it pretty much
      converges to the global minimum