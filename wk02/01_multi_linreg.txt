Multiple Features

Linear regression w/ multiple variables aka "multivariate linear regression"
    e.g. a house could have features like size of the house (x_1), # bedrooms (x_2), # floors (x_3), age (x_4) 
    to determine price (y)

Notation:
    x^(i) = a vector of the input (features) of the ith training example
    x_j^(i) = value of feature j in ith training example
    m = # training examples
    n = # features

Multivariate form of the hypothesis function (h_θ(x) = θ_0 + θ_1x) for multiple features:
    h(x) = θ_0 + θ_1x_1 + θ_2x_2 + θ_3x_3 + ... + θ_nx_n (for convenience, x_0 = 1, therefore x_0^(i) = 1)
    h(x) = [θ_0 θ_1 ... θ_n] * [x_0] = θ^Tx
                               [x_1]
                               [...]
                               [x_n]

Ex. Features of a house
    θ_0 = basic price of a house
    θ_1 = price per square meter
    x_1 = # square meters in house
    θ_2 = price per floor
    x_2 = # floors

    h(x) = 80 + 0.1x_1 + 0.01x_2 + 3x_3 - 2x_4 (x_4 = age of house, so value goes down the older it gets)

==================================================

Gradient Descent for Multiple Variables

Hypothesis: h(x) = θ^Tx
Parameters: θ (n+1-dimensional vector)
Cost function: J(θ) = 1/(2m) * sum(i=1, m) (h_θ(x^(i)) - y^(i))^2

Algorithm for linear regression:
    repeat until convergence: {
        θ_0 := θ_0 - α * 1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i))
        θ_1 := θ_1 - α * 1/m * sum(i=1, m) ((h_θ(x^(i)) - y^(i)) * x^(i))
    }

Algorithm for n features:
    repeat until convergence: {
        θ_0 := θ_0 - α * 1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_0^(i)
        θ_1 := θ_1 - α * 1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_1^(i)
        θ_2 := θ_2 - α * 1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_2^(i)
        ...
    }

    in other words...

        θ_j := θ_j - α * 1/m * sum(i=1, m) (h_θ(x^(i)) - y^(i)) * x_j^(i) for j := 0...n

==================================================

Gradient Descent in Practice I - Feature Scaling

You can speed up gradient descent by keeping input values in roughly the same range
    - θ will descend quickly on small ranges and slowly on large ranges
    - In a large range, θ oscillates inefficiently down to the global minimum when the variables are uneven

To prevent that, modify the ranges of the input variables/features x_i
    - Either -1 ≤ x_i ≤ 1 or -0.5 ≤ x_i ≤ 0.5
    - (Up to -3 ≤ x_i ≤ 3 or down to -1/3 ≤ x_i ≤ 1/3 is fine)

Feature scaling
    Divides the input values by the range (max value - min value) of the input variable, 
    resulting in a new range of 1

Mean normalization
    Subtracts the avg value for an input variable from the values for that input variable, 
    resulting in a new average value of 0.

To implement both techniques:
    x_i := (x_i - µ_i) / s_i
    µ_i = avg of all values for feature i
    s_i = range of values (max - min), or standard deviation

==================================================

Gradient Descent in Practice II - Learning Rate (α)

θ_j := θ_j - α * ∂/∂θ_j * J(θ)

The job of GD is to find θ that minimizes the cost function J(θ).
If you plot # iterations on x-axis against J(θ) on y-axis, you ideally want a monotonically decreasing convex graph. 

Debugging gradient descent
    Plot with # iterations on x-axis, cost function (J(θ)) on y-axis
    If J(θ) ever increases, then you need to decrease α.

Automatic convergence test
    Let E be some small value, such as 10^-3 (could be any small value, can be hard to pick)
    If J(θ) decreases by less than E in one iteration, then declare convergence.

If α is too small, slow convergence.
If α is too big, J(θ) may not decrease every iteration, may not lead to convergence.

When choosing α, try ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...

==================================================

Features and Polynomial Regressions

Ex. Housing prices prediction
    h(x) = θ_0 + θ_1 * frontage + θ_2 * depth
        where x_1 = frontage, x_2 = depth
    
    - Maybe I can decide what really matters is the area, which is frontage * depth
    - Then your hypothesis function turns to: 
    
    h(x) = θ_0 + θ_1x where x is the land area.

Combining multiple features into one
    - E.g. combine x_1 and x_2 into a new feature x_3 by taking x_1 * x_2

Polynomial regression
    - Hypothesis function doesn't need to be linear if it doesn't fit the data well
    - Can change behavior of curve by making it quadratic, cubic, square root etc.
    - E.g. h(x) = θ_0 + θ_1x_1 + θ_2(x_1)^2 or h(x) = θ_0 + θ_1x_1 + θ_2(x_1)^2 + θ_3(x_1)^3
        - in the cubic example, we created new features where x_2 = (x_1)^2 and x_3 = (x_1)^3
    - Could also make h(x) = θ_0 + θ_1x_1 + θ_2*sqrt(x_1)

Keep in mind how your features will scale
    - If x_1 has range 1-1000, then (x_1)^2 has range 1-1000000 and (x_1)^3 has range 1-1000000000