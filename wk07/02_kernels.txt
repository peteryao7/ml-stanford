Kernels I

Adapting SVMs to develop complex nonlinear classifiers

Say we want to predict y=1 if 
θ_0 + θ_1x_1 + θ_2x_2 + θ_3x_1*x_2 + θ_4x_1^2 + θ_5x_2^2 + ... >= 0
h_θ(x) = 1 if the polynomial >= 0
         0 otherwise

Another way of writing this is:
θ_0 + θ_1f_1 + θ_2f_2 + θ_3f_3 + ...
where f_1 = x_1, f_2 = x_2, f_3 = x_1*x_2, f_4 = x_1^2, f_5 = x_2^2 ...

Is there a different/better choice of the features f_1, f_2, f_3, ...?

-----

Given x, compute new features depending on proximity to landmarks l^(1), l^(2), l^(3)

    |   . l^(1)
    |
x_2 |           . l^(2)
    |
    |       . l^(3)
    |
    ------------------
            x_1

Given x: f_1 = similarity(x, l^(1)) = exp(-(||x-l^(1)||^2) / (2*σ^2))
         f_2 = similarity(x, l^(2)) = exp(-(||x-l^(2)||^2) / (2*σ^2))
         ...

The similarity and exp functions are called (Gaussian) kernels, denoted by k(x,l^(i))

-----

Take f_1 = similarity(x, l^(1)) = exp(-(||x-l^(1)||^2) / (2*σ^2))
                                = exp(-(sum(j=1, n) (x_j - l_j^1)^2) / (2*σ^2))
    
If x ≈ l^(1), then f_1 ≈ exp(-0^2/(2*σ^2)) ≈ 1
If x is far from l^(1), then f_1 = exp (-(large number)^2/(2*σ^2)) ≈ 0

Each of these landmarks l^(1), l^(2), l^(3) define new features f_1, f_2, f_3 respectively.

-----

Example:
l^(1) = [3; 5], f_1 = exp(-||x-l^(1)||^2 / (2*σ^2)), σ^2 = 1

If you graph the contour plot, then at [3; 5] f_1 takes on the value 1, then as x goes
further away from the features, it approaches closer to 0.

σ^2 is a parameter of the Gaussian kernel. If we vary it, we can get different effects.
If we set σ^2 = 0.5, the kernel looks similar, but the bump in the contour becomes narrower.
    - if we start at [3; 5] and move away, then the feature falls to 0 faster.
If we set σ^2 = 3, the bump becomes wider
    - if we start at [3; 5] and move away, the features falls a lot more slowly

Note that f_1 stays at 1 as its maximum.

-----

Say we want to predict "1" when θ_0 + θ_1f_1 + θ_2f_2 + θ_3f_3 >= 0
Let θ_0 = -0.5, θ_1 = 1, θ_2 = 1, θ_3 = 0

Then f_1 ≈ 1, f_2 ≈ 0, f_3 ≈ 0 since the training example is close to l_1 and far from l_2 and l_3
θ_0 + θ_1*1 + θ_2*0 + θ_3*0
= -0.5 + 1 = 0.5 >= 0
So we'd predict y = 1.

==================================================

Kernels II

Where do we get landmarks like l^(1), l^(2), l^(3) from?

Take some training examples, and put your landmarks in the same locations at those examples.
You'll end up with m landmarks, from l^(1), l^(2)... up to l^(m).

Your features will measure how close an example is to one of the things you saw in your training set.

-----

Given (x^(1), y^(1)), (x^(2), y^(2)), ..., (x^(m), y^(m))
choose l^(1) = x^(1), l^(2) = x^(2), ..., l^(m) = x^(m)

Given example x:
f_1 = similarity(x, l^(1)) where l^(1) = x^(1)
f_2 = similarity(x, l^(2))
...

As a feature vector:
f = [f_1; f_2; ...; f_m] (can also add an f_0 = 1)

For training example (x^(i), y^(i)):
         f_1^(i) = sim(x^(i), l^(1))
x^(i) -> f_2^(i) = sim(x^(i), l^(2))
         ...
         f_m^(i) = sim(x^(i), l^(m))

where x^(i) ∈ R^(n+1) with f_0^(i) = 1

Somewhere in the middle, f_i^(i) = sim(x^(i), l^(i)) = exp(-0/(2*σ^2)) = 1

-----

SVM with Kernels

Hypothesis: Given x, compute features f ∈ R^(m+1)
    Predict "y=1" if θ^T*f >= 0
    where θ^T*f = θ_0*f_0 + θ_1*f_1 + ... + θ_m*f_m

Training:
    min θ C * [sum(i=1, m) y^(i) * cost_1(θ^T*f^(i)) + (1 - y^(i)) * cost_0(θ^T*f^(i))] 
          + 1/2 * sum(j=1, n) θ_j^2
    (here, n is equal to the # of features of the training example, or m in the previous slide)

The last term is actually done slightly differently:
    - another way to write it is θ^T*θ, if θ = [θ_1; θ_2; ...; θ_m]
    - sometimes, it's written as θ^T*M*θ where M is a matrix depending on the kernel you use,
      to evaluate a slightly different distance metric
    - if M is huge, like 50000 or 100000, then solving for all these parameters becomes expensive
      for the SVM

Using kernels with other algorithms like logistic regression will be really slow, so SVM isn't
used for much else other than classification.

-----

Choosing SVM parameters: 

C (= 1/λ)
Large C (small λ): lower bias, high variance -> overfitting
Small C (large λ): higher bias, low variance -> underfitting

σ^2
Large σ^2: features f_i vary more smoothly (hill)
           higher bias, lower variance -> underfitting
Small σ^2: features f_i vary less smoothly (peak)
           lower bias, higher variance -> overfitting