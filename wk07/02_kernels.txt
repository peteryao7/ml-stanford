Kernels I

Adapting SVMs to develop complex nonlinear classifiers

Say we want to predict y=1 if 
θ_0 + θ_1x_1 + θ_2x_2 + θ_3x_1*x_2 + θ_4x_1^2 + θ_5x_2^2 + ... >= 0
h_θ(x) = 1 if the polynomial >= 0
         0 otherwise

Another way of writing this is:
θ_0 + θ_1f_1 + θ_2f_2 + θ_3f_3 + ...
where f_1 = x_1, f_2 = x_2, f_3 = x_1*x_2, f_4 = x_1^2, f_5 = x_2^2 ...

Is there a different/better choice of the features f_1, f_2, f_3, ...?

-----

Given x, compute new features depending on proximity to landmarks l^(1), l^(2), l^(3)

    |   . l^(1)
    |
x_2 |           . l^(2)
    |
    |       . l^(3)
    |
    ------------------
            x_1

Given x: f_1 = similarity(x, l^(1)) = exp(-(||x-l^(1)||^2) / (2*σ^2))
         f_2 = similarity(x, l^(2)) = exp(-(||x-l^(2)||^2) / (2*σ^2))
         ...

The similarity and exp functions are called (Gaussian) kernels, denoted by k(x,l^(i))

-----

Take f_1 = similarity(x, l^(1)) = exp(-(||x-l^(1)||^2) / (2*σ^2))
                                = exp(-(sum(j=1, n) (x_j - l_j^1)^2) / (2*σ^2))
    
If x ≈ l^(1), then f_1 ≈ exp(-0^2/(2*σ^2)) ≈ 1
If x is far from l^(1), then f_1 = exp (-(large number)^2/(2*σ^2)) ≈ 0

Each of these landmarks l^(1), l^(2), l^(3) define new features f_1, f_2, f_3 respectively.

-----

Example:
l^(1) = [3; 5], f_1 = exp(-||x-l^(1)||^2 / (2*σ^2)), σ^2 = 1

If you graph the contour plot, then at [3; 5] f_1 takes on the value 1, then as x goes
further away from the features, it approaches closer to 0.

σ^2 is a parameter of the Gaussian kernel. If we vary it, we can get different effects.
If we set σ^2 = 0.5, the kernel looks similar, but the bump in the contour becomes narrower.
    - if we start at [3; 5] and move away, then the feature falls to 0 faster.
If we set σ^2 = 3, the bump becomes wider
    - if we start at [3; 5] and move away, the features falls a lot more slowly

Note that f_1 stays at 1 as its maximum.

-----

Say we want to predict "1" when θ_0 + θ_1f_1 + θ_2f_2 + θ_3f_3 >= 0
Let θ_0 = -0.5, θ_1 = 1, θ_2 = 1, θ_3 = 0

Then f_1 ≈ 1, f_2 ≈ 0, f_3 ≈ 0 since the training example is close to l_1 and far from l_2 and l_3
θ_0 + θ_1*1 + θ_2*0 + θ_3*0
= -0.5 + 1 = 0.5 >= 0
So we'd predict y = 1.