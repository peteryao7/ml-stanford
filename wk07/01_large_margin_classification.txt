Optimization Objective

Support Vector Machine (SVM)
    - supervisory learning algorithm
    - sometimes gives a cleaner and more powerful way of learning complex
      non-linear functions
    
-----

Alternate view of logistic regression

h_θ(x) = 1 / (1 + e^(-θ^Tx)), z = θ^Tx

If y = 1, we want h_θ(x) ≈ 1, θ^Tx >> 0
If y = 0, we want h_θ(x) ≈ 0, θ^Tx << 0

Cost of example: -(y*log(h_θ(x)) + (1 - y) * log(1 - h_θ(x)))
    substitute h_θ(x) with 1 / (1 + e^(-θ^Tx))

If y = 1, then only the first term really matters as the other will be 0.
Then the function ends up being -log(1 / (1 + e^-z)), which is a decreasing concave function.
To build a SVM, we'll take that function and modify it.
    - z on the x-axis from 1 onwards will be 0
    - then when z < 1, there is a straight line with a negative slope

If y = 0, then only the second term really matters as the other will be 0.
Then the function ends up being -log(1 - 1 / (1 + e^-z)), which is an increasing concave function.
To build an SVM, we'll modify that function again.
    - z from -1 and before will be 0
    - then when z > -1, there is a straight line with a positive slope

For the SVM:
min θ C * [sum(i=1, m) y^i * cost_1(θ^Tx^(i)) + (1 - y^i) * cost_0(θ^Tx^(i))] 
+ λ/2 * sum(i=0, n) θ_j^2

Firstly, we can reparametize this to ignore both 1/m terms since they're just constants, 
we'd end up with the same optimal value for theta regardless. 

Ex. for min u (u - 5)^2 + 1             => u = 5
If we multiplied the function by 10,
min u 10*(u - 5)^2 + 10                 => u = 5

Secondly, we can separate the objective function into 2 terms, the cost term as A
and the regularization term as B:
A + λB
We can use a different term C instead and write it as:
CA + B
For logistic regression, if we set λ to be a large value, we gave B more weight. Here, 
we want to find the minimum value of C to give B more weight.
You can think of C = 1/λ, but they're not the same.

-----

SVM hypothesis

min θ C * [sum(i=1, m) y^i * cost_1(θ^Tx^(i)) + (1 - y^i) * cost_0(θ^Tx^(i))] 

h_θ(x) will predict 1 if θ^Tx >= 0
                    0 otherwise

==================================================

Large Margin Intuition

min θ C * [sum(i=1, m) y^i * cost_1(θ^Tx^(i)) + (1 - y^i) * cost_0(θ^Tx^(i))] 

If y = 1, we want θ^Tx >= 1 (not just >= 0)
If y = 0, we want θ^Tx <= -1 (not just < 0)
These are the places where cost(z) = 0

-----

If C is large, say 100000:

We want to choose a value for the first term that will set it to 0

We want to set the cost to 0, so:
    Whenever y^(i) = 1, θ^Tx^(i) >= 1
    Whenever y^(i) = 0, θ^Tx^(i) <= -1
min θ C * 0 + 1/2 * sum(i=1, n) θ_j^2 with those constraints

When you solve this optimization problem and get a decision boundary, you may
not get one that looks right. It could separate the positive and negative examples
perfectly, but the slope may look really unnatural.

The SVM would choose a much more natural looking decision boundary. It has a minimum
distance from the other training examples, which is the margin of the SVM, and gives
it a certain robustness as it tries to find the boundary with the maximum margin.

-----

Large margin classifier with outliers

SVM is more than just the large margin classifier. If you only used the LMC, you will
be susceptible to outliers, and the model would try to fit and separate them. 

This tells you that C plays a similar role to λ, where if C is too big or too small,
you could end up with boundaries that try to overfit outliers.

==================================================

The Mathematics Behing Large Margin Classification

Vector Inner Product

Let u = [u_1; u_2] and v = [v_1; v_2]
u^Tv = ?
||u|| = length of vector u
      = sqrt(u_1^2 + u_2^2) ∈ R

Let p be the length of the projection of v onto u
Then u^Tv (= v^Tu) = p * ||u||
          = u_1*v_1 + u_2*v_2

Note that p ∈ R and ||u|| ∈ R and p is signed, so if the angle between u and v is > 90 deg,
then we'd get a projection that goes in the opposite direction of u, resulting in a -p.

-----

SVM decision boundary

min θ 1/2 * sum(j=1, n) θ_j^2 = 1/2 * (θ_1^2 + θ_2^2) = 1/2 * (sqrt(θ_1^2 + θ_2^2))^2 = ||θ||
    (sqrt(θ_1^2 + θ_2^2) is just the norm of θ)
s.t. θ^T*x^(i) >= 1 if y^(i) = 1 and θ^T*x^(i) <= -1 if y^(i) = 0
    - θ^T*x^(i) looks similar to u^T*v, so let θ^T = u^T and x(i) = v
    - Then we can plot x^(i) as (x_1^(i), x_2^(i)) on a plane, and get a vector pointing 
      to the training example
    - We can also plot θ as (θ_1, θ_2) on the same plane, and get another vector pointing to θ
    - Now we can take the inner product of them, and get:
        θ^T*x^(i) = p^(i) * ||θ||
                  = θ_1*x_1^(i) + θ_2*x_2^(i)

Now we can replace those constraints as:
min θ 1/2 * sum(j=1, n) θ_j^2 = 1/2 * ||θ||^2
s.t. p^(i) * ||θ|| >= 1 if y^(i) = 1 and p^(i) * ||θ|| <= -1 if y^(i) = 0
where p^(i) is the projection of x^(i) onto the vector θ.

(Simplification: θ_0 = 0, meaning θ must pass through the origin)

-----

Suppose the SVM chose a decision boundary with tiny margins. Why wouldn't SVM do this?

If we illustrate the boundary, θ is perpendicular to the boundary at the origin.
For some training example x^(1), the distance from p^(1) might be small, but for some x^(2) on the
other side, then p^(2), going in the opposite direction, could also be small. But this also means |p|
will be pretty small, and for p^(1) * ||θ|| >= 1 to be true, then ||θ|| must be large. Additionally,
for p^(2) * ||θ|| <= -1, then ||θ|| must also be large.

For another boundary, it might be vertical, so θ is perpendicular and is on the x-axis. p^(1) and
p^(2) are now much bigger, so the constraints are easier to fulfill now that ||θ|| can be smaller. 
SVM can now make the norm of the parameters θ much smaller, therefore ||θ||^2 will also be smaller.
SVM will pick this boundary instead.

We want our projections to be large, which act as our margins.